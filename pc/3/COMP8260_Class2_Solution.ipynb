{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8ff1fa",
   "metadata": {},
   "source": [
    "# COMP8260 - AI Systems Class 2\n",
    "### Jupyter Notebook Solution\n",
    "\n",
    "This notebook provides a structured solution to the tasks outlined in the class PDF document. Each task includes explanations, implementation, and answers to relevant questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f6f19c",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Dataset\n",
    "We begin by loading the **Adult Dataset (version 2)** from `openml.org` using `fetch_openml`. We will explore the dataset structure, feature types, missing values, and target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch the dataset\n",
    "dataset = fetch_openml(data_id=1590, as_frame=True)\n",
    "\n",
    "# Extract data and target\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(X.info())\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(X.isnull().sum())\n",
    "\n",
    "# Dataset size\n",
    "print(\"\\nDataset Size:\", X.shape)\n",
    "\n",
    "# Target class distribution\n",
    "print(\"\\nTarget Distribution:\")\n",
    "print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe8dbb",
   "metadata": {},
   "source": [
    "## 2. Split Data into Training and Test Sets\n",
    "We will split the dataset into **80% training and 20% testing** using `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display shapes\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca77c17",
   "metadata": {},
   "source": [
    "## 3. Select Numerical Features\n",
    "We extract only **numerical features** for training the first Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select numerical features\n",
    "X_train_num = X_train.select_dtypes(include=['int64', 'float64'])\n",
    "X_test_num = X_test.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Display shapes\n",
    "print(\"X_train_num Shape:\", X_train_num.shape)\n",
    "print(\"X_test_num Shape:\", X_test_num.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf8fbb",
   "metadata": {},
   "source": [
    "## 4. Train a DecisionTreeClassifier on Numerical Data\n",
    "We train a `DecisionTreeClassifier` using default parameters and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train Decision Tree\n",
    "clf_num = DecisionTreeClassifier()\n",
    "clf_num.fit(X_train_num, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_num = accuracy_score(y_train, clf_num.predict(X_train_num))\n",
    "test_accuracy_num = accuracy_score(y_test, clf_num.predict(X_test_num))\n",
    "\n",
    "# Print results\n",
    "print(\"Training Accuracy:\", train_accuracy_num)\n",
    "print(\"Testing Accuracy:\", test_accuracy_num)\n",
    "print(\"Tree Depth:\", clf_num.get_depth())\n",
    "print(\"Number of Leaves:\", clf_num.get_n_leaves())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2610b5",
   "metadata": {},
   "source": [
    "## 8. Optimize Decision Tree with GridSearchCV\n",
    "We use `GridSearchCV` to find the best values for `max_depth`, `min_samples_split`, and `min_samples_leaf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, n_jobs=-1, return_train_score=True)\n",
    "grid_search.fit(X_train_num, y_train)\n",
    "\n",
    "# Display best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba77fb",
   "metadata": {},
   "source": [
    "## 9. Implement a Pipeline with the Best Parameters\n",
    "We create a pipeline to preprocess categorical features and apply the best decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cfe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Select categorical features\n",
    "categorical_features = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "\n",
    "# Define categorical preprocessing pipeline\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Create pipeline using best params\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(**grid_search.best_params_, random_state=42))\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate pipeline\n",
    "train_accuracy = pipeline.score(X_train, y_train)\n",
    "test_accuracy = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Pipeline Training Accuracy:\", train_accuracy)\n",
    "print(\"Pipeline Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b15780",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Conclusion\n",
    "- We successfully trained and optimized a **Decision Tree Classifier**.\n",
    "- **Grid Search** helped find the best hyperparameters.\n",
    "- **Pipelines** were used to process categorical data and automate model training.\n",
    "- The final model was evaluated for accuracy and overfitting."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
